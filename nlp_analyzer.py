#!/usr/bin/env python3
import sqlite3
import requests
import spacy
import time

class NLPAnalyzer:
    def __init__(self, api_key, db_path="devto_metrics.db"):
        self.api_key = api_key
        self.headers = {"api-key": api_key}
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        
        # Chargement du modÃ¨le spaCy (Anglais par dÃ©faut pour la tech)
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except:
            print("ExÃ©cute d'abord : python3 -m spacy download en_core_web_sm")

    def sync_markdown_bodies(self):
        """RÃ©cupÃ¨re le Markdown depuis l'API pour les commentaires qui ne l'ont pas encore"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT comment_id FROM comments WHERE body_markdown IS NULL OR body_markdown = ''")
        rows = cursor.fetchall()
        
        if not rows:
            return

        print(f"ğŸ”„ Synchronisation de {len(rows)} commentaires (Markdown)...")
        for row in rows:
            c_id = row['comment_id']
            res = requests.get(f"https://dev.to/api/comments/{c_id}")
            if res.status_code == 200:
                md = res.json().get('body_markdown', '')
                cursor.execute("UPDATE comments SET body_markdown = ? WHERE comment_id = ?", (md, c_id))
                self.conn.commit()
                time.sleep(0.2) # Courtoisie API

    def extract_insights(self):
        """Analyse sÃ©mantique des commentaires des lecteurs"""
        cursor = self.conn.cursor()
        # On exclut tes propres commentaires pour l'analyse d'audience
        cursor.execute("""
            SELECT article_title, body_markdown 
            FROM comments 
            WHERE author_username != 'pascal_cescato_692b7a8a20' 
            AND body_markdown IS NOT NULL
        """)
        
        print(f"\nğŸ§  INSIGHTS SÃ‰MANTIQUES (Lecteurs)")
        print("="*100)
        
        for row in cursor.fetchall():
            doc = self.nlp(row['body_markdown'])
            # Extraction des noms propres et entitÃ©s techniques (ORG/PRODUCT)
            tech_entities = [ent.text for ent in doc.ents if ent.label_ in ["ORG", "PRODUCT"]]
            
            if tech_entities:
                print(f"\nArt: {row['article_title'][:50]}...")
                print(f"  â”” Tags dÃ©tectÃ©s: {', '.join(set(tech_entities))}")

    def show_thematic_summary(self):
        """Regroupe les insights par article pour une vision stratÃ©gique"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT article_title, GROUP_CONCAT(body_markdown, ' || ') as all_comments
            FROM comments 
            WHERE author_username != 'pascal_cescato_692b7a8a20'
            GROUP BY article_title
        """)
        
        print(f"\nğŸ“‘ SYNTHÃˆSE THÃ‰MATIQUE PAR ARTICLE")
        print("="*100)
        
        for row in cursor.fetchall():
            doc = self.nlp(row['all_comments'])
            # On filtre les mots-clÃ©s techniques et les concepts frÃ©quents
            keywords = [token.lemma_.lower() for token in doc 
                        if token.pos_ in ['NOUN', 'PROPN'] 
                        and not token.is_stop and len(token.text) > 2]
            
            # On prend les 5 plus frÃ©quents
            from collections import Counter
            common = Counter(keywords).most_common(5)
            tags = ", ".join([word for word, count in common])
            
            print(f"\nğŸ“˜ {row['article_title'][:60]}...")
            print(f"   ğŸ” Sujets chauds : {tags}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 nlp_analyzer.py VOTRE_CLE_API")
    else:
        analyzer = NLPAnalyzer(sys.argv[1])
        analyzer.sync_markdown_bodies()
        analyzer.extract_insights()